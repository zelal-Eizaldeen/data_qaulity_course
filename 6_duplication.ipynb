{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEGXj0HdX3y1VZhnfxV0ZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelal-Eizaldeen/data_qaulity_course/blob/main/6_duplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX6FZs_18bsA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a sample dataset with employee information. We’ll intentionally introduce duplicate employee IDs to demonstrate the identification of duplicates:"
      ],
      "metadata": {
        "id": "K-TZBse38euD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'EmployeeID': [101, 102, 103, 101, 104, 105, 102],\n",
        "    'FirstName': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Bob'],\n",
        "    'LastName': ['Smith', 'Johnson', 'Brown', 'Davis', 'Lee', 'White', 'Johnson'],\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "r3vM-aMA8ckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use pandas to identify and mark duplicate records based on the EmployeeID column. The duplicated() function is used to create a Boolean mask, where True indicates a duplicated record:"
      ],
      "metadata": {
        "id": "11xctCPi8nUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated_mask = df.duplicated(subset='EmployeeID', keep='first')\n"
      ],
      "metadata": {
        "id": "1E9e6M2L8p3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The subset='EmployeeID' argument specifies the column on which the duplication check is performed. keep='first' marks duplicates as True except for the first occurrence. You can change this parameter to last or False based on your requirements."
      ],
      "metadata": {
        "id": "i-t1GH_z8xhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a new column called 'IsDuplicate' in the DataFrame to indicate whether each record is a duplicate or not:"
      ],
      "metadata": {
        "id": "-zUrs8b1802f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['IsDuplicate'] = duplicated_mask"
      ],
      "metadata": {
        "id": "fzEMG1lM830m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the percentage of duplicate records by dividing the number of duplicate records (those marked as True in the 'IsDuplicate' column) by the total number of records and then multiplying by 100 to express it as a percentage:"
      ],
      "metadata": {
        "id": "GGPhPOIB86J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_percentage = (df['IsDuplicate'].sum() / len(df)) * 100\n"
      ],
      "metadata": {
        "id": "mkJgs-jl8_NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we display the dataset with the IsDuplicate column to see which records are duplicates. Here’s the final output:"
      ],
      "metadata": {
        "id": "F86HI-6F9Btm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)\n",
        "print(f\"Percentage of Duplicate Records: {duplicate_percentage}\")"
      ],
      "metadata": {
        "id": "DMAPu3I59CTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output indicates that 28.57% of records are duplicated in the dataset."
      ],
      "metadata": {
        "id": "S7iYTaLi9TwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The fewer duplicated records, the better!\n",
        "- The threshold for what is considered an acceptable or “good” level of duplicated records in a dataset can vary depending on the specific context and the goals of your data management or analysis. There is no one-size-fits-all answer to this question, as it depends on factors such as the type of data, the purpose of the dataset, and industry standards."
      ],
      "metadata": {
        "id": "4KrJRBun9WCJ"
      }
    }
  ]
}